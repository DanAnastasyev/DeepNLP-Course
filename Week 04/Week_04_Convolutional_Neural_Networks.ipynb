{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 04 - Convolutional Neural Networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Z8LbImWBw4Zd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip install -qq bokeh==0.13.0\n",
        "!pip install -qq eli5==0.8\n",
        "!wget -O surnames.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1z7avv1JiI30V4cmHJGFIfDEs9iE4SHs5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dF1fio53UKN6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "    \n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q3lN5pl5Stpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Свёрточные нейронные сети"
      ]
    },
    {
      "metadata": {
        "id": "9J6KBv-cniLw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Классификация фамилий\n",
        "\n",
        "Будем учиться предсказывать, является ли слово фамилией."
      ]
    },
    {
      "metadata": {
        "id": "7f2Sk0mMFJeW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('surnames.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    data = [line.strip().split('\\t')[0] for line in lines]\n",
        "    labels = np.array([int(line.strip().split('\\t')[1]) for line in lines])\n",
        "    del lines\n",
        "    \n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfUIgqMznrXB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотрим на данные:"
      ]
    },
    {
      "metadata": {
        "id": "YN1XYen8UauD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list(zip(train_data, train_labels))[::1500]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uFlOaCNt3fuV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Данные ещё и сильно несбалансированы - положительных примеров в несколько раз меньше:"
      ]
    },
    {
      "metadata": {
        "id": "ikKB2DaK08-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "positive_count = np.sum(train_labels == 1)\n",
        "negative_count = len(train_labels) - positive_count\n",
        " \n",
        "plt.bar(np.arange(2), [negative_count, positive_count], align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(2), ('Negative', 'Positive'))\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4MwgOy5M8qPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Accuracy очень легко оптимизировать - просто предсказывайте всегда ноль:"
      ]
    },
    {
      "metadata": {
        "id": "UuqFWWu38UwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Accuracy = {:.2%}'.format((train_labels == 0).mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LtOFSmy93pDl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Однако это будет довольно бесполезно - всегда говорить, что слово не является фамилией. Это, конечно, вопрос - что хуже, зря объявить слово фамилией (ошибка первого рода) или не найти фамилию.\n",
        "\n",
        "![](https://effectsizefaq.files.wordpress.com/2010/05/type-i-and-type-ii-errors.jpg =x350)  \n",
        "*From [effectsizefaq.com](https://effectsizefaq.com/2010/05/31/i-always-get-confused-about-type-i-and-ii-errors-can-you-show-me-something-to-help-me-remember-the-difference/)*\n",
        "\n",
        "Будем замерять precision, recall и их комбинацию - $F_1$-меру.\n",
        "\n",
        "![precision-recall](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png =x600)  \n",
        "*From [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall)*\n",
        "\n",
        "$$\\text{precision} = \\frac{tp}{tp + fp}.$$\n",
        "$$\\text{recall} = \\frac{tp}{tp + fn}.$$\n",
        "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}.$$"
      ]
    },
    {
      "metadata": {
        "id": "9bl6O_bAtVqd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Начнём с бейзлайна на регулярках."
      ]
    },
    {
      "metadata": {
        "id": "Q6tUhBLUy0Wf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Супер-бейзлайн\n",
        "surname_indicators = \"^[А-Я][а-я], .*ский\" #@param {type:\"raw\"}\n",
        "\n",
        "surname_indicators = surname_indicators.split(', ')\n",
        "\n",
        "import re\n",
        "\n",
        "regexs = [re.compile(regex) for regex in surname_indicators]\n",
        "\n",
        "preds = np.array([any(regex.match(word) for regex in regexs) for word in test_data])\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print('F1-score = {:.2%}'.format(f1_score(test_labels, preds)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJJtiGMK1ATy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А теперь серьёзно - бейзлайн на логистической регрессии поверх N-грамм символов.\n",
        "\n",
        "**Задание** Сделать классификацию с LogisticRegression моделью. Посчитать F1-меру."
      ]
    },
    {
      "metadata": {
        "id": "8COAoh7b0TXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)\n",
        "\n",
        "X_train, X_test = <convert data>\n",
        "\n",
        "model = <fit LogisticRegression>\n",
        "\n",
        "test_preds = model.predict(X_test)\n",
        "print('F1-score = {:.2%}'.format(f1_score(test_labels, test_preds)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vN46GwPrzeJW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотрим на предсказания"
      ]
    },
    {
      "metadata": {
        "id": "F-P8OSD7yZQt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "\n",
        "eli5.show_weights(model, vec=vectorizer, top=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iYXljseDyd77",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample_ind = np.random.randint(len(test_data))\n",
        "eli5.show_prediction(model, test_data[sample_ind], vec=vectorizer, targets=['surname'], target_names=['word', 'surname'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cQF3JlD7twQK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Кроме тупого подсчета F1-score можно посмотреть на precision-recall кривые. Во-первых, они красивые. Во-вторых, по ним видно, что можно повысить качество (F1-score), подобрав другой порог - **хотя на тесте это делать нельзя**."
      ]
    },
    {
      "metadata": {
        "id": "a7oyvJ-2EG0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(test_labels, model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "f_scores = np.linspace(0.2, 0.8, num=4)\n",
        "lines = []\n",
        "labels = []\n",
        "for f_score in f_scores:\n",
        "    x = np.linspace(0.01, 1)\n",
        "    y = f_score * x / (2 * x - f_score)\n",
        "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
        "    plt.annotate('F1 = {0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
        "\n",
        "plt.plot(recall, precision)\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tL6Bnl8ftoYP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Придумайте признаки, чтобы улучшить качество модели."
      ]
    },
    {
      "metadata": {
        "id": "rRhcCJcAS4S5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Character-Level Convolutions\n",
        "\n",
        "### Общее описание сверток\n",
        "\n",
        "Напомню, свертки - это то, с чего начался хайп нейронных сетей в районе 2012-ого.\n",
        "\n",
        "Работают они примерно так:  \n",
        "![Conv example](https://image.ibb.co/e6t8ZK/Convolution.gif)   \n",
        "From [Feature extraction using convolution](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution).\n",
        "\n",
        "Формально - учатся наборы фильтров, каждый из которых скалярно умножается на элементы матрицы признаков. На картинке выше исходная матрица сворачивается с фильтром\n",
        "$$\n",
        " \\begin{pmatrix}\n",
        "  1 & 0 & 1 \\\\\n",
        "  0 & 1 & 0 \\\\\n",
        "  1 & 0 & 1\n",
        " \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Но нужно не забывать, что свертки обычно имеют ещё такую размерность, как число каналов. Например, картинки имеют обычно три канала: RGB.  \n",
        "Наглядно демонстрируется как выглядят при этом фильтры [здесь](http://cs231n.github.io/convolutional-networks/#conv).\n",
        "\n",
        "После сверток обычно следуют pooling-слои. Они помогают уменьшить размерность тензора, с которым приходится работать. Самым частым является max-pooling:  \n",
        "![maxpooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg =x300)  \n",
        "From [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/#pool)"
      ]
    },
    {
      "metadata": {
        "id": "x-M3lCE1ealB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Свёртки для текстов\n",
        "\n",
        "Для текстов свертки работают как n-граммные детекторы (примерно). Каноничный пример символьной сверточной сети:\n",
        "\n",
        "![text-convs](https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png =x500)  \n",
        "From [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)\n",
        "\n",
        "*Сколько учится фильтров на данном примере?*\n",
        "\n",
        "На картинке показано, как из слова извлекаются 2, 3 и 4-граммы. Например, желтые - это триграммы. Желтый фильтр прикладывают ко всем триграммам в слове, а потом с помощью global max-pooling извлекают наиболее сильный сигнал.\n",
        "\n",
        "Что это значит, если конкретнее?\n",
        "\n",
        "Каждый символ отображается с помощью эмбеддингов в некоторый вектор. А их последовательности - в конкатенации эмбеддингов.  \n",
        "Например, \"abs\" $\\to [v_a; v_b; v_s] \\in \\mathbb{R}^{3 d}$, где $d$ - размерность эмбеддинга. Желтый фильтр $f_k$ имеет такую же размерность $3d$.  \n",
        "Его прикладывание - это скалярное произведение $\\left([v_a; v_b; v_s] \\odot f_k \\right) \\in \\mathbb R$ (один из желтых квадратиков в feature map для данного фильтра).\n",
        "\n",
        "Max-pooling выбирает $max_i \\left( [v_{i-1}; v_{i}; v_{i+1}] \\odot f_k \\right)$, где $i$ пробегается по всем индексам слова от 1 до $|w| - 1$ (либо по большему диапазону, если есть padding'и).   \n",
        "Этот максимум соответствует той триграмме, которая наиболее близка к фильтру по косинусному расстоянию.\n",
        "\n",
        "В результате в векторе после max-pooling'а закодирована информация о том, какие из n-грамм встретились в слове: если встретилась близкая к нашему $f_k$ триграмма, то в $k$-той позиции вектора будет стоять большое значение, иначе - маленькое.\n",
        "\n",
        "А учим мы как раз фильтры. То есть сеть должна научиться определять, какие из n-грамм значимы, а какие - нет."
      ]
    },
    {
      "metadata": {
        "id": "xz7ApwLZ05kc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Игрушечный пример\n",
        "\n",
        "Посмотрим на примере, что там происходит. Возьмем слово:"
      ]
    },
    {
      "metadata": {
        "id": "P7fbpo630iZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word = 'Смирнов'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1R4gvJVD0-0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Для начала нужно перенумеровать символы:"
      ]
    },
    {
      "metadata": {
        "id": "x99Q65ot02Vi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "char2index = {symb: ind for ind, symb in enumerate(set(word))}\n",
        "\n",
        "char2index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3sxJYZEs1M8y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Каждому символу сопоставляется эмбеддинг. Простейший способ сделать эмбеддинги - взять единичную матрицу. Когда у нас были десятки тысяч слов, такие эмбеддинги были не оч, а сейчас всего несколько символов вполне адекватно присвоить им ортогональные вектора небольшой размерности."
      ]
    },
    {
      "metadata": {
        "id": "wbApeJdb1EGq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = torch.eye(len(char2index))\n",
        "\n",
        "embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mP9i5pPx2BGs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим тензор индексов символов слова:"
      ]
    },
    {
      "metadata": {
        "id": "PBIIlHy91ohP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_tensor = torch.LongTensor([char2index[symb] for symb in word])\n",
        "\n",
        "word_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eL3qLw-p2Htk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Отобразим его в эмбеддинги. Получили такой же прямоугольничек, как на картинке (транспонирование нужно, чтобы смотрело в ту же сторону)."
      ]
    },
    {
      "metadata": {
        "id": "4fkChxtt1yqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_embs = embeddings[word_tensor].t()\n",
        "\n",
        "word_embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QL4o1EI-2O_I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь дело дошло до сверток. Сделаем фильтр-детектор триграммы `нов`:"
      ]
    },
    {
      "metadata": {
        "id": "ToW8KCUY1723",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kernel_name = 'нов'\n",
        "\n",
        "kernel_indices = torch.LongTensor([char2index[symb] for symb in kernel_name])\n",
        "kernel_weights = embeddings[kernel_indices].t()\n",
        "\n",
        "kernel_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kTxfchOy4c9w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Чтобы посчитать свёртку, воспользуемся функцией:\n",
        "\n",
        "```python\n",
        "F.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
        "```\n",
        "\n",
        "input: input tensor of shape ($N \\times C_{in} \\times H_{in} \\times W_{in}$)  \n",
        "weight: filters of shape ($C_{out} \\times C_{in} \\times H_{out} \\times W_{out}$)\n",
        "\n",
        "$N$ - размер батча (1 у нас). $C_{in}$ - число каналов. В нашем случае оно всегда будет 1 (пока что). $C_{out}$ - число фильтров. Оно пока 1.\n",
        "\n",
        "Нам понадобятся четырехмерные тензоры, для этого воспользуемся `view`:"
      ]
    },
    {
      "metadata": {
        "id": "Oyd3Na7V3JMr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_embs = word_embs.view(1, 1, word_embs.shape[0], word_embs.shape[1])\n",
        "kernel_weights = kernel_weights.view(1, 1, kernel_weights.shape[0], kernel_weights.shape[1])\n",
        "\n",
        "conv_result = F.conv2d(word_embs, kernel_weights)[0, 0]\n",
        "\n",
        "print('Conv =', conv_result)\n",
        "print('Max pooling =', conv_result.max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVAiQ2PG6SOd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Свертка сказала, что данный фильтр есть на последней позиции. Пулинг сказал, пофиг на какой позиции - главное, он есть."
      ]
    },
    {
      "metadata": {
        "id": "h7ZVbNOGoV5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Подготовка данных\n",
        "\n",
        "Первый шаг - определить, какой длины слова у нас. Ограничимся каким-то числом, а более длинные будем обрезать."
      ]
    },
    {
      "metadata": {
        "id": "DEtZ6g78Wtj-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter \n",
        "    \n",
        "def find_max_len(counter, threshold):\n",
        "    sum_count = sum(counter.values())\n",
        "    cum_count = 0\n",
        "    for i in range(max(counter)):\n",
        "        cum_count += counter[i]\n",
        "        if cum_count > sum_count * threshold:\n",
        "            return i\n",
        "    return max(counter)\n",
        "\n",
        "word_len_counter = Counter()\n",
        "for word in train_data:\n",
        "    word_len_counter[len(word)] += 1\n",
        "    \n",
        "threshold = 0.99\n",
        "MAX_WORD_LEN = find_max_len(word_len_counter, threshold)\n",
        "\n",
        "print('Max word length for {:.0%} of words is {}'.format(threshold, MAX_WORD_LEN))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MdKoOBdE8uj4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Соберем отображение из символов в индексы."
      ]
    },
    {
      "metadata": {
        "id": "YyMoPEXGVs3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chars = set()\n",
        "for word in train_data:\n",
        "    chars.update(word)\n",
        "\n",
        "char_index = {c : i + 1 for i, c in enumerate(chars)}\n",
        "char_index['<pad>'] = 0\n",
        "    \n",
        "print(char_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4spzGF2CImtL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Сконвертируйте данные"
      ]
    },
    {
      "metadata": {
        "id": "qETmYKm8W_TX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_data(data, max_word_len, char_index):\n",
        "    return <np array>\n",
        "\n",
        "X_train = convert_data(train_data, MAX_WORD_LEN, char_index)\n",
        "X_test = convert_data(test_data, MAX_WORD_LEN, char_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VufrP006Vk-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def iterate_batches(X, y, batch_size):\n",
        "    num_samples = X.shape[0]\n",
        "\n",
        "    indices = np.arange(num_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        end = min(start + batch_size, num_samples)\n",
        "        \n",
        "        batch_idx = indices[start: end]\n",
        "        \n",
        "        yield X[batch_idx], y[batch_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkXlKB7lobYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MyFirstConvCharNN\n",
        "\n",
        "Теперь построим свёрточную модель.\n",
        "\n",
        "Пусть она будет строить триграммы - то есть применять фильтры на 3 символа.\n",
        "\n",
        "Начнем с последовательности: `nn.Embedding -> nn.Conv2d -> nn.ReLU -> max pooling -> nn.Linear`\n",
        "\n",
        "`nn.Conv2d` - это слой, содержащий создание и инициализацию фильтров, и вызов `F.conv2d` к ним и входу.\n",
        "\n",
        "*Лайфхак:* последовательности операций можно запаковывать в `nn.Sequential`."
      ]
    },
    {
      "metadata": {
        "id": "v2T4AorsZ530",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, filters_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._embedding = ...\n",
        "        self._dropout = nn.Dropout(0.2)\n",
        "        self._conv3 = ...\n",
        "        self._out_layer = ...\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        '''\n",
        "        inputs - LongTensor with shape (batch_size, max_word_len)\n",
        "        outputs - FloatTensor with shape (batch_size,)\n",
        "        '''\n",
        "        \n",
        "        outputs = self.embed(inputs)\n",
        "        return self._out_layer(outputs).squeeze(-1)\n",
        "    \n",
        "    def embed(self, inputs):\n",
        "        <calc word embedding>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0iTDc9rEAHN6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проверьте, что всё работает:"
      ]
    },
    {
      "metadata": {
        "id": "WiAdrSaU_wjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_batch, y_batch = next(iterate_batches(X_train, train_labels, 32))\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "model = ConvClassifier(len(char_index) + 1, 24, 64)\n",
        "logits = model(X_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x39p9w-tA_Ds",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Подсчитайте precision, recall и F1-score для полученных предсказаний."
      ]
    },
    {
      "metadata": {
        "id": "eVg2Ws7mA40a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "<calc precision, recall, f1-score>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1TFsKlF9BV8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Напишем теперь цикл обучения, который не слишком сложно будет переиспользовать"
      ]
    },
    {
      "metadata": {
        "id": "MsCtTJucVjMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
        "    epoch_loss, epoch_tp, epoch_fp, epoch_fn = 0, 0, 0, 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    model.train(is_train)\n",
        "    \n",
        "    data, labels = data\n",
        "    batchs_count = math.ceil(data.shape[0] / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size)):\n",
        "            X_batch, y_batch = LongTensor(X_batch), FloatTensor(y_batch)\n",
        "\n",
        "            logits = <calc logits>\n",
        "\n",
        "            loss = <calc loss>\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if is_train:\n",
        "                <how to optimize the beast?>\n",
        "\n",
        "            <u can move the stuff to some function>\n",
        "            tp = <calc true positives>\n",
        "            fp = <calc false positives>\n",
        "            fn = <calc false negatives>\n",
        "\n",
        "            precision = ...\n",
        "            recall = ...\n",
        "            f1 = ...\n",
        "            \n",
        "            epoch_tp += tp\n",
        "            epoch_fp += fp\n",
        "            epoch_fn += fn\n",
        "\n",
        "            print('\\r[{} / {}]: Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'.format(\n",
        "                  i, batchs_count, loss.item(), precision, recall, f1), end='')\n",
        "        \n",
        "    precision = ...\n",
        "    recall = ...\n",
        "    f1 = ...\n",
        "        \n",
        "    return epoch_loss / batchs_count, recall, precision, f1\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
        "        batch_size=32, val_data=None, val_batch_size=None):\n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_recall, train_precision, train_f1 = do_epoch(\n",
        "            model, criterion, train_data, batch_size, optimizer\n",
        "        )\n",
        "        \n",
        "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
        "        if not val_data is None:\n",
        "            val_loss, val_recall, val_precision, val_f1 = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            output_info += ', Val Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
        "                                     train_loss, train_recall, train_precision, train_f1,\n",
        "                                     val_loss, val_recall, val_precision, val_f1))\n",
        "        else:\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlq63hAXh0Gr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = ConvClassifier(len(char_index) + 1, 24, 128).cuda()\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().cuda()\n",
        "\n",
        "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad])\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, train_labels), epochs_count=200, \n",
        "    batch_size=512, val_data=(X_test, test_labels), val_batch_size=1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4o-1AEcFCjAk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Проверьте работу классификатора на вашей фамилии.\n",
        "\n",
        "Нужно не забыть перевести модель в режим инференса - некоторые слои на трейне и инференсе ведут себя по-разному."
      ]
    },
    {
      "metadata": {
        "id": "svO9OrF4CiLI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "surname = \"...\"\n",
        "surname_tensor = ...\n",
        "print('P({} is surname) = {}'.format(surname, torch.sigmoid(model(surname_tensor))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "djkksZKcDPA1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Постройте precision-recall curve для данного классификатора и предыдущей модели"
      ]
    },
    {
      "metadata": {
        "id": "PZm7O56pDcH_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_FPTfBzSDheP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Визуализации\n",
        "\n",
        "### Визуализация эмбеддингов\n",
        "\n",
        "**Задание** Визуализируем эмбеддинги слов, как это делали раньше"
      ]
    },
    {
      "metadata": {
        "id": "DeW5av_ASrn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "    \n",
        "    if isinstance(color, str): \n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: \n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=100)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "    \n",
        "    \n",
        "def visualize_embeddings(embeddings, token, colors):\n",
        "    tsne = get_tsne_projection(embeddings)\n",
        "    draw_vectors(tsne[:, 0], tsne[:, 1], color=colors, token=token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67KNX5lBTdrt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_indices = np.random.choice(np.arange(len(test_data)), 1000, replace=False)\n",
        "words = [test_data[ind] for ind in word_indices]\n",
        "labels = test_labels[word_indices]\n",
        "\n",
        "word_tensor = convert_data(words, max(len(x) for x in words), char_index)\n",
        "embeddings = <calc embeddings>\n",
        "\n",
        "colors = ['red' if label else 'blue' for label in labels]\n",
        "\n",
        "visualize_embeddings(embeddings, words, colors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xOt1tcOkvDBY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Визуализация полученных свёрток"
      ]
    },
    {
      "metadata": {
        "id": "tljoUWceE2lH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Кроме всего прочего у нас тут логистическая регрессия сверху. Можно визуализировать ее также, как в eli5.\n",
        "\n",
        "**Задание** Добиться этого."
      ]
    },
    {
      "metadata": {
        "id": "mJpHT_YkLvCf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "word = 'Смирнов'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNMXpgh8FNIo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посчитайте вероятность, что слово - фамилия."
      ]
    },
    {
      "metadata": {
        "id": "Hl-VZs1mFKxM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputs = word -> LongTensor\n",
        "prob = torch.sigmoid(model(inputs)).item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y2SZi1FIFj6o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посчитайте результат свертки и пулинга"
      ]
    },
    {
      "metadata": {
        "id": "-1SdKbDFFhOk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "convs = ...\n",
        "maxs, positions = convs.squeeze().max(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JLTu8mlfFtXJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Домножьте выход пулинга на веса выходного слоя"
      ]
    },
    {
      "metadata": {
        "id": "ZRMkbFXdFwys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "linear_weights = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qV3dK-IzGFXh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посчитайте веса символов: каждый фильтр прикладывается к какой-то позиции - прибавим его вес к накрываемым символам."
      ]
    },
    {
      "metadata": {
        "id": "E0y8epEgGEU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "symb_weights = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "utVltLCUGXTy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Визуализируем это:"
      ]
    },
    {
      "metadata": {
        "id": "I2UKjz86F7fz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "\n",
        "def get_color_hex(weight):\n",
        "    cmap = plt.get_cmap(\"RdYlGn\")\n",
        "    rgba = cmap(weight, bytes=True)\n",
        "    return '#%02X%02X%02X' % rgba[:3]\n",
        "\n",
        "symb_template = '<span style=\"background-color: {color_hex}\">{symb}</span>'\n",
        "res = '<p>P(surname) = {:.2%}</p>'.format(prob)\n",
        "for symb, weight in zip(word, symb_weights):\n",
        "    res += symb_template.format(color_hex=get_color_hex(weight), symb=symb)\n",
        "res = '<p>' + res + '</p>'\n",
        "\n",
        "HTML(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mGhfZsleGZi0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Объединим все в функции:"
      ]
    },
    {
      "metadata": {
        "id": "PUi51X4fQIIT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_weights(word):\n",
        "    <calc>\n",
        "    \n",
        "    return prob, symb_weights\n",
        "\n",
        "def visualize(word):\n",
        "    prob, symb_weights = calc_weights(word)\n",
        "    \n",
        "    symb_template = '<span style=\"background-color: {color_hex}\">{symb}</span>'\n",
        "    res = '<p>P(surname) = {:.2%}</p>'.format(prob)\n",
        "    for symb, weight in zip(word, symb_weights):\n",
        "        res += symb_template.format(color_hex=get_color_hex(weight), symb=symb)\n",
        "    res = '<p>' + res + '</p>'\n",
        "    return HTML(res)\n",
        "\n",
        "\n",
        "visualize('Королев')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eXee135DEJuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Улучшение модели"
      ]
    },
    {
      "metadata": {
        "id": "k2_8bNRyG3CF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Для улучшение стабильности модели стоит добавить дропаут `nn.Dropout` - способ занулять часть весов на каждой эпохе для регуляризации модели. Попробуйте добавить его после эмбеддингов и после свертки (а можно еще где-нибудь).\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1044/1*iWQzxhVlvadk6VAJjsgXgg.png =x300)\n",
        "\n",
        "\n",
        "**Задание** Другой способ регуляризовывать модель - использовать BatchNormalization (`nn.BatchNorm2d`). Попробуйте добавить его после свертки.\n",
        "\n",
        "**Задание** Еще способ улучшить модель - добавить сверток. Реализуйте модель как на картинке в начале ноутбука: со свертками на 2, 3, 4 символа."
      ]
    },
    {
      "metadata": {
        "id": "qstoDysVsSQ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Различают Narrow и Wide свёртки - по сути, добавляется ли нулевой паддинг или нет. Для текстов эта разница выглядит так:  \n",
        "![narrow_vs_wide](https://image.ibb.co/eqGZaS/2018_03_28_11_23_17.png)\n",
        "*From Neural Network Methods in Natural Language Processing.*\n",
        "\n",
        "Слева - паддинг отсутствует, справа - есть. Попробуйте добавить паддинг и посмотреть, что получится. Потенциально он поможет выучить хорошие префиксы слова."
      ]
    },
    {
      "metadata": {
        "id": "v39_tgnMnr0J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Дополнительные материалы\n",
        "\n",
        "## Почитать\n",
        "\n",
        "### Основы\n",
        "[Convolutional Neural Networks, cs231n](http://cs231n.github.io/convolutional-networks/)  \n",
        "[Understanding Convolutions, Christopher Olah](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)  \n",
        "[Understanding Convolutional Neural Networks for NLP, Denny Britz](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
        "\n",
        "### Статьи\n",
        "[Character-Aware Neural Language Models, Yoon Kim et al, 2015](https://arxiv.org/abs/1508.06615)  \n",
        "[Character-level Convolutional Networks for Text Classification, Zhang et al., 2015](https://arxiv.org/abs/1509.01626)  \n",
        "[A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification Zhang et al., 2015](https://arxiv.org/abs/1510.03820)\n",
        "[Learning Character-level Representations for Part-of-Speech Tagging, dos Santos et al, 2014](http://proceedings.mlr.press/v32/santos14.pdf)\n",
        "\n",
        "## Посмотреть\n",
        "[cs224n \"Lecture 13: Convolutional Neural Networks\"](https://www.youtube.com/watch?v=Lg6MZw_OOLI)"
      ]
    },
    {
      "metadata": {
        "id": "5JwcvGNSIJs4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача задания\n",
        "\n",
        "[Форма](https://goo.gl/forms/FfMnyNGI2P4xo0QD3)"
      ]
    }
  ]
}
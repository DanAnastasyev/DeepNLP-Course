{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 08 - Language Models (Part 2).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OE7fXh-OSJYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip -qq install torchtext==0.3.1\n",
        "!pip -qq install gensim==3.6.0\n",
        "!pip -qq install pyldavis==2.1.2\n",
        "!pip -qq install attrs==18.2.0\n",
        "!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1OIU9ICMebvZXJ0Grc2SLlMep3x9EkZtz' -O perashki.txt\n",
        "!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1v66uAEKL3KunyylYitNKggdl2gCeYgZZ' -O poroshki.txt\n",
        "!git clone https://github.com/UniversalDependencies/UD_Russian-SynTagRus.git\n",
        "!wget -qq https://raw.githubusercontent.com/DanAnastasyev/neuromorphy/master/neuromorphy/train/corpus_iterator.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uhvfH55PUJ8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "txWqIO_74A4s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word-Level Text Generation"
      ]
    },
    {
      "metadata": {
        "id": "KOD_3I7d4oDV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Сегодня занимаемся, в основном, тем, что генерируем *пирожки* и *порошки*.\n",
        "\n",
        "*(Данные без спросу скачаны с сайта http://poetory.ru)*\n",
        "\n",
        "Пирожки - это вот:"
      ]
    },
    {
      "metadata": {
        "id": "d2vMrlrRQpuJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head perashki.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Lm0-PeG5Dh9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Порошки вот:"
      ]
    },
    {
      "metadata": {
        "id": "2-Jf88bxVTGj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head poroshki.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AgYh4FNP5FyX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Не перепутайте!\n",
        "\n",
        "Вообще, пирожок - это четверостишие, написанное четырехстопным ямбом по схеме 9-8-9-8. У порошка схема 9-8-9-2."
      ]
    },
    {
      "metadata": {
        "id": "bSBpLFRgGaXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vowels = 'ёуеыаоэяию'\n",
        "\n",
        "odd_pattern = '-+-+-+-+-'\n",
        "even_pattern = '-+-+-+-+'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hl9BFoug519c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Считываем данные:"
      ]
    },
    {
      "metadata": {
        "id": "O3aFzzOQKLlD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_poem(path):\n",
        "    poem = []\n",
        "    with open(path, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            if len(line) == 0:\n",
        "                yield poem\n",
        "                poem = []\n",
        "                continue\n",
        "            \n",
        "            poem.extend(line.split() + ['\\\\n'])\n",
        "            \n",
        "perashki = list(read_poem('perashki.txt'))\n",
        "poroshki = list(read_poem('poroshki.txt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xiRq1vbf55qN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим датасет для порошков:"
      ]
    },
    {
      "metadata": {
        "id": "ZOBgLAgVTrk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
        "\n",
        "text_field = Field(init_token='<s>', eos_token='</s>')\n",
        "        \n",
        "fields = [('text', text_field)]\n",
        "examples = [Example.fromlist([poem], fields) for poem in poroshki]\n",
        "dataset = Dataset(examples, fields)\n",
        "\n",
        "text_field.build_vocab(dataset, min_freq=7)\n",
        "\n",
        "print('Vocab size =', len(text_field.vocab))\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
        "                                              shuffle=True, device=DEVICE, sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8FYJe2CA8GcY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Напишите класс языковой модели."
      ]
    },
    {
      "metadata": {
        "id": "x8ndCRZLl4ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=256, lstm_hidden_dim=256, num_layers=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
        "        \n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "        \n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self, init_range=0.1):\n",
        "        self._emb.weight.data.uniform_(-init_range, init_range)\n",
        "        self._out_layer.bias.data.zero_()\n",
        "        self._out_layer.weight.data.uniform_(-init_range, init_range)\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        <apply layers>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ySJ4tUAqvFvB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_qVuSL8QJg4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "\n",
        "model(batch.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rsh3_eR08PqQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Добавьте подсчет потерей с маскингом паддингов."
      ]
    },
    {
      "metadata": {
        "id": "_E2JxfRuphch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "tqdm.get_lock().locks = []\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = len(data_iter)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, batch in enumerate(data_iter):                \n",
        "                logits, _ = model(batch.text)\n",
        "\n",
        "                <calc loss>\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "                    optimizer.step()\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
        "                                                                                         math.exp(loss.item())))\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
        "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
        "            )\n",
        "            progress_bar.refresh()\n",
        "\n",
        "    return epoch_loss / batches_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
        "    best_val_loss = None\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_iter is None:\n",
        "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n",
        "            \n",
        "            if best_val_loss and val_loss > best_val_loss:\n",
        "                optimizer.param_groups[0]['lr'] /= 4.\n",
        "                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n",
        "            else:\n",
        "                best_val_loss = val_loss\n",
        "        print()\n",
        "        generate(model)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ufpoSwQ-8bcN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Напишите функцию-генератор для модели."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BYoHY1se2bhB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(probs, temp):\n",
        "    probs = F.log_softmax(probs.squeeze(), dim=0)\n",
        "    probs = (probs / temp).exp()\n",
        "    probs /= probs.sum()\n",
        "    probs = probs.cpu().numpy()\n",
        "\n",
        "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
        "\n",
        "\n",
        "def generate(model, temp=0.6):\n",
        "    model.eval()\n",
        "    with torch.no_grad():        \n",
        "        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n",
        "        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n",
        "        \n",
        "        hidden = None\n",
        "        for _ in range(150):\n",
        "            <generate text>\n",
        "                \n",
        "generate(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5X2kYDU_rCjP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "\n",
        "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
        "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
        "criterion = nn.CrossEntropyLoss(...).to(DEVICE)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n",
        "\n",
        "fit(model, criterion, optimizer, train_iter, epochs_count=300, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r_YtM4ms8v--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Добавьте маскинг `<unk>` токенов при тренировке модели."
      ]
    },
    {
      "metadata": {
        "id": "LzGwmgVf9Dkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Улучшаем модель"
      ]
    },
    {
      "metadata": {
        "id": "BHneb8br9WXh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tying input and output embeddings\n",
        "\n",
        "В модели есть два эмбеддинга - входной и выходной. Красивая и полезная в жизни идея - учить только одну матрицу, расшаренную между ними: [Using the Output Embedding to Improve Language Models](http://www.aclweb.org/anthology/E17-2025).\n",
        "\n",
        "От идеи одни плюсы: получается намного меньше обучаемых параметров и при этом достаточно заметно более высокое качество.\n",
        "\n",
        "**Задание** Реализуйте это. Достаточно написать что-то типа этого в конструкторе:\n",
        "\n",
        "`self._out_layer.weight = self._emb.weight`"
      ]
    },
    {
      "metadata": {
        "id": "N8I3QC4a_a8q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Добавление информации в выборку\n",
        "\n",
        "Сейчас у нас каждое слово предствляется одним индексом. Модели очень сложно узнать, сколько в нем слогов - а значит, сложно генерировать корректное стихотворение.\n",
        "\n",
        "На самом деле к каждому слову можно приписать кусочек из метрического шаблона:\n",
        "\n",
        "![](https://hsto.org/web/59a/b39/bd0/59ab39bd020c49a78a12cbab62c80181.png =x200)\n",
        "\n",
        "**Задание** Обновите функцию `read_poem`, пусть она генерирует два списка - список слов и список кусков шаблона.  \n",
        "Добавьте в модель вход - последовательности шаблонов, конкатенируйте их эмбеддинги со словами.  \n",
        "Дополнительная идея - заставьте модель угадывать, какой шаблон должен идти следующим (где-то половина будет подходящими, остальные - нет). Добавьте дополнительные потери от угадывания шаблона."
      ]
    },
    {
      "metadata": {
        "id": "MBX4NjzZ-0Hc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Увеличиваем выборку\n",
        "\n",
        "У нас есть выборка для пирожков, которая заметно больше.\n",
        "\n",
        "**Задание** Обучитесь на ней.\n",
        "\n",
        "### Transfer learning\n",
        "\n",
        "Простой и приятный способ улучшения модели - сделать перенос обученной на большом корпусе модели на меньшего объема датасет.\n",
        "\n",
        "Популярен этот способ больше в компьютерном зрении: [Transfer learning, cs231n](http://cs231n.github.io/transfer-learning/) - там есть огромный ImageNet, на котором предобучают модель, чтобы потом заморозить нижние слои и заменить выходные. В итоге модель использует универсальные представления данных, выученные на большом корпусе, но для предсказания совсем других меток - и качество очень здорово растет.\n",
        "\n",
        "Нам такие извращения пока не нужны (хотя потом пригодятся, ключевые слова: ULMFiT, ELMo и компания). Просто возьмем обученную на большем корпусе модель и поучим ее на меньшем корпусе. Ей всего-то нужно новый матрический шаблон последней строки выучить.\n",
        "\n",
        "**Задание** Обученную в прошлом пункте модель дообучите на порошки.\n",
        "\n",
        "### Conditional language model\n",
        "\n",
        "Ещё лучше - просто учиться на обоих корпусах сразу. Объедините пирожки и порошки, для каждого храните индекс 0/1 - был ли это пирожок или порошок. Добавьте вход - этот индекс и конкатенируйте его либо к каждому эмбеддингу слов, либо к каждому выходу из LSTM.\n",
        "\n",
        "**Задание** Научите единую модель, у которой можно просить сгенерировать пирожок или порошок."
      ]
    },
    {
      "metadata": {
        "id": "WnP743CM-bY6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Variational & word dropout\n",
        "\n",
        "**Задание** На прошлом занятии приводились примеры более приспособленных к RNN'ам dropout'ов. Добавьте их.\n",
        "\n",
        "**Задание** Кроме этого, попробуйте увеличивать размер модели или количество слоев в ней, чтобы улучшить качество."
      ]
    },
    {
      "metadata": {
        "id": "Ejqx6BC0JcG2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multi-task learning\n",
        "\n",
        "Ещё один важный способ улучшения модели - multi-task learning. Это когда одна модель учится делать предсказания сразу для нескольких задач.\n",
        "\n",
        "В нашем случае это может быть предсказанием отдельно леммы слова и отдельно - его грамматического значения:\n",
        "![](https://hsto.org/web/e97/8a8/6e8/e978a86e8a874d8d946bb15e6a49a713.png =x350)\n",
        "\n",
        "В итоге модель выучивает как языковую модель по леммам, так и модель POS tagging'а. Одновременно!\n",
        "\n",
        "Возьмем корпус из universal dependencies - он уже размечен, как нужно.\n",
        "\n",
        "Почитаем его:"
      ]
    },
    {
      "metadata": {
        "id": "YT-kzC2_KuLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from corpus_iterator import Token, CorpusIterator\n",
        "\n",
        "fields = [('word', Field()), ('lemma', Field()), ('gram_val', Field())]\n",
        "examples = []\n",
        "\n",
        "with CorpusIterator('UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu') as corpus_iter:\n",
        "    for sent in corpus_iter:\n",
        "        words = ['<s>'] + [tok.token.lower() for tok in sent] + ['</s>']\n",
        "        lemmas = ['<s>'] + [tok.lemma.lower() for tok in sent] + ['</s>']\n",
        "        gr_vals = ['<s>'] + [tok.grammar_value for tok in sent] + ['</s>']\n",
        "        examples.append(Example.fromlist([words, lemmas, gr_vals], fields))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_3xaD-2KwNW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Words:', examples[1].word)\n",
        "print('Lemmas:', examples[1].lemma)\n",
        "print('Grammar vals:', examples[1].gram_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HcGm5fPsLESH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Таким образом, размер словаря может быть существенно сокращен - лемм меньше, чем слов, а предсказание грамматики вынуждает модель быть более осведомленной о согласовании слов."
      ]
    },
    {
      "metadata": {
        "id": "xZe5HimdLb9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = Dataset(examples, fields)\n",
        "\n",
        "dataset.fields['word'].build_vocab(dataset, min_freq=3)\n",
        "print('Word vocab size =', len(dataset.fields['word'].vocab))\n",
        "dataset.fields['lemma'].build_vocab(dataset, min_freq=3)\n",
        "print('Lemma vocab size =', len(dataset.fields['lemma'].vocab))\n",
        "dataset.fields['gram_val'].build_vocab(dataset)\n",
        "print('Grammar val vocab size =', len(dataset.fields['gram_val'].vocab))\n",
        "\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=0.75)\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
        "                                              shuffle=True, device=DEVICE, sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y7xlr15lLm78",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим маппинг из пары (лемма, грамматическое значение) в слово - если бы у нас под рукой был морфологический словарь, маппинг можно было бы пополнить, добавить слова для лемм из корпуса, которые не встретились в обучении."
      ]
    },
    {
      "metadata": {
        "id": "_AvT2MgeLmP8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dictionary = {\n",
        "    (lemma, gr_val): word\n",
        "    for example in train_iter.dataset.examples \n",
        "    for word, lemma, gr_val in zip(example.word, example.lemma, example.gram_val)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VaP8Krx1LeJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание**  Обновите генератор - например, можно сэмплировать лемму и находить самое вероятное грамматическое значение, которое встречается  в паре с этой леммой в `dictionary`."
      ]
    },
    {
      "metadata": {
        "id": "PeBH0WYjMQ5h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate(model, temp=0.7):\n",
        "    ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3GzOZ8dMVMJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Обновите модель и функцию обучения.\n",
        "\n",
        "Модель должна принимать пары `lemma, gr_val`, конкатенировать их эмбеддинги и предсказывать следующие `lemma, gr_val` по выходу из LSTM.\n",
        "\n",
        "Функция `do_epoch` должна суммировать потери по предсказанию леммы (делая маскинг для `<unk>` и `<pad>`) + потери по предсказанию грамматического значения (с маскингом по `<pad>`)."
      ]
    },
    {
      "metadata": {
        "id": "vL2xPe-BNRhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Контролируемая генерация\n",
        "\n",
        "Хочется сделать генерацию более контролируемой - в идеале, задавать тему.\n",
        "\n",
        "Простой способ - сделать тематическое моделирование и найти в текстах какие-то темы - а потом передавать вектор тем вместе с эмбеддингом слова, чтобы модель училась генерировать тематически-согласованный текст."
      ]
    },
    {
      "metadata": {
        "id": "exopH1jlN4fc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "docs = [[word for word in poem if word != '\\\\n'] for poem in perashki]\n",
        "\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "dictionary.filter_n_most_frequent(100)\n",
        "\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "lda_model = models.LdaModel(bow_corpus, num_topics=5, id2word=dictionary, passes=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLPO9U1-Pakp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотреть, что выучилось, можно так:"
      ]
    },
    {
      "metadata": {
        "id": "hzEg-8SZs8t7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHx1GJrWPkM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Предсказывает распределение модель как-то так:"
      ]
    },
    {
      "metadata": {
        "id": "rTD0CGMdPsF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for word in perashki[10]:\n",
        "    if word == '\\\\n':\n",
        "        print()\n",
        "    else:\n",
        "        print(word, end=' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0m0b6i2MPlKD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lda_model.get_document_topics(bow_corpus[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-imKaGGUQM5K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Посчитайте для всех текстов вектора тем, передавайте их вместе со словами (конкатенируя к эмбеддингам). Посмотрите, вдруг чего получится."
      ]
    },
    {
      "metadata": {
        "id": "w8V0KAz_CNf0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Дополнительные материалы\n",
        "\n",
        "## Статьи\n",
        "\n",
        "Regularizing and Optimizing LSTM Language Models, 2017 [[arxiv]](https://arxiv.org/abs/1708.02182), [[github]](https://github.com/salesforce/awd-lstm-lm) - одна из самых полезных статей про языковые модели + репозиторий, в котором реализовано много полезного, стоит заглянуть\n",
        "\n",
        "Exploring the Limits of Language Modeling, 2016 [[arxiv]](https://arxiv.org/abs/1602.02410)\n",
        "\n",
        "Using the Output Embedding to Improve Language Models, 2017 [[pdf]](http://www.aclweb.org/anthology/E17-2025)\n",
        "\n",
        "\n",
        "## Transfer learning\n",
        "[Transfer learning, cs231n](http://cs231n.github.io/transfer-learning/)  \n",
        "[Transfer learning, Ruder](http://ruder.io/transfer-learning/) - очень подробная статья от чувака из NLP\n",
        "\n",
        "## Multi-task learning\n",
        "[An Overview of Multi-Task Learning in Deep Neural Networks, Ruder](http://ruder.io/multi-task/)  \n",
        "[Multi-Task Learning Objectives for Natural Language Processing, Ruder](http://ruder.io/multi-task-learning-nlp/)"
      ]
    },
    {
      "metadata": {
        "id": "Vwb5e5hPQebd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача\n",
        "\n",
        "[Форма для сдачи](https://goo.gl/forms/ASLLjYncKUcIHmuO2)  \n",
        "[Feedback](https://goo.gl/forms/9aizSzOUrx7EvGlG3)"
      ]
    }
  ]
}
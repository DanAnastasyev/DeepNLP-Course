{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OlqOAQmQGXOL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -O imdb.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vrQ5czMHoO3pEnmofFskymXMkq_u1dPc\"\n",
        "!unzip imdb.zip\n",
        "!pip -q install eli5\n",
        "!pip -q install spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uF80-qhXt17P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Тут придется авторизировать запрос к гугл драйву, извините."
      ]
    },
    {
      "metadata": {
        "id": "lRdS0CM2rSzV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1d1-5FwxK53ePwygNWeG7jhsOWZbi5HOv'})\n",
        "downloaded.GetContentFile('train_docs.pkl')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1MMOY477t965G0C5DtXeREVp0X85UaNq5'})\n",
        "downloaded.GetContentFile('test_docs.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Icl_tExut9XX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Организационное\n",
        "\n",
        "Страница курса: https://github.com/DanAnastasyev/DeepNLP-Course\n",
        "\n",
        "Чатик: https://t.me/joinchat/DgDT4RLYhsTh_x6fRvfMjQ\n",
        "\n",
        "Примерный план:\n",
        "\n",
        "1. Введение\n",
        "2. PyTorch, word embeddings\n",
        "3. RNN 1, text classification\n",
        "4. RNN 2, sequence labeling\n",
        "5. RNN 3, text generation, language models\n",
        "6. CNN, text classification\n",
        "7. Machine translation, attention\n",
        "8. Transformers\n",
        "9. Dialogue systems 1, tagging + classification, multi-task learning\n",
        "10. Dialogue systems 2, DSSM\n",
        "11. Transfer learning\n",
        "12. Tensorflow + xla/aot or Meta BiLSTM (?)\n",
        "\n",
        "... and something else"
      ]
    },
    {
      "metadata": {
        "id": "kqbkG4qoS0AB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Классификация текстов\n",
        "\n",
        "Начнём с самого простого - анализа тональности текста.\n",
        "\n",
        "Будем классифицировать отзывы с IMDB на положительные/отрицательные.\n",
        "\n",
        "Датасет взят с http://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "metadata": {
        "id": "Js5kqmWMGqN4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head train.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5R9e3pctHIV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"train.tsv\", delimiter=\"\\t\")\n",
        "test_df = pd.read_csv(\"test.tsv\", delimiter=\"\\t\")\n",
        "\n",
        "print('Train size = {}'.format(len(train_df)))\n",
        "print('Test size = {}'.format(len(test_df)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6oHbJCPPwhjp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотрите глазами на тексты? Какие есть зацепки, как определить, что это за сентимент?\n",
        "\n",
        "Самое простое, как всегда - найти ключевые слова."
      ]
    },
    {
      "metadata": {
        "id": "DywUCyMLr_TD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Начинаем классифицировать! { vertical-output: true, display-mode: \"form\" }\n",
        "positive_words = 'love', 'great', 'best', 'wonderful' #@param {type:\"raw\"}\n",
        "negative_words = 'worst', 'awful', '1/10', 'crap' #@param {type:\"raw\"}\n",
        "\n",
        "positives_count = test_df.review.apply(lambda text: sum(word in text for word in positive_words))\n",
        "negatives_count = test_df.review.apply(lambda text: sum(word in text for word in negative_words))\n",
        "is_positive = positives_count > negatives_count\n",
        "correct_count = (is_positive == test_df.is_positive).values.sum()\n",
        "\n",
        "accuracy = correct_count / len(test_df)\n",
        "\n",
        "print('Test accuracy = {:.2%}'.format(accuracy))\n",
        "if accuracy > 0.71:\n",
        "    from IPython.display import Image, display\n",
        "    display(Image('https://s3.amazonaws.com/achgen360/t/rmmoZsub.png', width=500))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oo8HRABxv1kW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Придумайте хорошие ключевые слова или фразы и наберите хотя бы 71% точности на тесте (и не забудьте посмотреть на код классификации!)"
      ]
    },
    {
      "metadata": {
        "id": "eaIrBClMUHZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Кому-нибудь нравятся эти `<br /><br />`? Лично мне - нет. Напишите регулярку, которая будет их удалять"
      ]
    },
    {
      "metadata": {
        "id": "09OkUmtde6ny",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "pattern = re.compile(<smth>)\n",
        "\n",
        "print(train_df['review'].iloc[3])\n",
        "print(pattern.subn(' ', train_df['review'].iloc[3])[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vO6D9NuMi4II",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Применим ее:"
      ]
    },
    {
      "metadata": {
        "id": "7LTwQqs_hD-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df['review'] = train_df['review'].apply(lambda text: pattern.subn(' ', text)[0])\n",
        "test_df['review'] = test_df['review'].apply(lambda text: pattern.subn(' ', text)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vGxzf4oXmXqw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Пора переходить к машинке!\n",
        "\n",
        "Как будем представлять текст? Проще всего - мешком слов.\n",
        "\n",
        "Заведём большой-большой словарь - список всех слов в обучающей выборке. Тогда каждое предложение можно представить в виде вектора, в котором будет записано, сколько раз встретилось каждое из возможных слов:\n",
        "\n",
        "![bow](https://raw.githubusercontent.com/DanAnastasyev/DeepNLP-Course/master/Week%2001/Images/BOW.png)\n",
        "\n",
        "Простой и приятный способ сделать это - запихнуть тексты в `CountVectorizer`.\n",
        "\n",
        "Он имеет такую сигнатуру:\n",
        "\n",
        "```python\n",
        "CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=r'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64'>)\n",
        "```\n",
        "\n",
        "Для начала обратим внимание на параметры `lowercase=True` и `max_df=1.0, min_df=1, max_features=None` - они про то, что по умолчанию все слова будут приводиться к нижнему регистру и в словарь попадут все слова, встречавшиеся в текстах.\n",
        "\n",
        "При желании можно было бы убрать слишком редкие или слишком частотные слова - пока не будем этого делать.\n",
        "\n",
        "Посмотрим на простом примере, как он будет работать:"
      ]
    },
    {
      "metadata": {
        "id": "5Odnum4iyGDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "dummy_data = ['The movie was excellent',\n",
        "              'the movie was awful']\n",
        "\n",
        "dummy_matrix = vectorizer.fit_transform(dummy_data)\n",
        "\n",
        "print(dummy_matrix.toarray())\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q3zC1ItWybVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Как именно vectorizer определяет границы слов? Обратите внимание на параметр `token_pattern=r'(?u)\\b\\w\\w+\\b'` - как он будет работать?*\n",
        "\n",
        "Запустим его на реальных данных:"
      ]
    },
    {
      "metadata": {
        "id": "Ccd2gaCdQq2W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_df['review'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J_JC9n6C0bFR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотрим на слова, попавшие в словарь:"
      ]
    },
    {
      "metadata": {
        "id": "stV4ICO3mKsf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oesbuQ9g0krj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Попробуем кого-нибудь таки сконвертировать"
      ]
    },
    {
      "metadata": {
        "id": "lUWtDWcp0g7U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer.transform([train_df['review'].iloc[3]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lZCRef3pyCRC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "То, что и хотели - вектор с bow (т.е. bag-of-words) представлением исходного текста.\n",
        "\n",
        "И чем эта информация может помочь? Ну, всё тем же - какие-то слова носят положительный окрас, какие-то - отрицательный. Большинство вообще нейтральный, да.\n",
        "\n",
        "![bow with weights](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2001/Images/BOW_weights.png)\n",
        "\n",
        "Хочется, наверное, подобрать коэффициенты, которые будут определять уровень окраса, да? Подбирать нужно по обучающей выборке, а не как мы перед этим делали.\n",
        "\n",
        "Например, для выборки\n",
        "```\n",
        "1   The movie was excellent\n",
        "0   the movie was awful\n",
        "```\n",
        "легко подобрать коэффициенты на глазок: что-нибудь вроде `+1` для `excellent`,  `-1` для `awful` и по нулям всем остальным.\n",
        "\n",
        "Построим линейную модель, которая станет этим заниматься. Она будет учиться строить разделяющую гиперплоскость в пространстве bow-векторов.\n",
        "\n",
        "Проверим, как справится логистическая регрессия с нашей супер-выборкой из пары предложений"
      ]
    },
    {
      "metadata": {
        "id": "i6WVgK4LtUn2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "dummy_data = ['The movie was excellent',\n",
        "              'the movie was awful']\n",
        "dummy_labels = [1, 0]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(dummy_data, dummy_labels)\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "print(classifier.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C9Y-kq-tv-XY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Получилось что надо.\n",
        "\n",
        "Запустим теперь её на реальных данных."
      ]
    },
    {
      "metadata": {
        "id": "kvxHIIbSiUXq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_df['review'], train_df['is_positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-d3BBV_uUu-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def eval_model(model, test_df):\n",
        "    preds = model.predict(test_df['review'])\n",
        "    print('Test accuracy = {:.2%}'.format(accuracy_score(test_df['is_positive'], preds)))\n",
        "    \n",
        "eval_model(model, test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pnmmz3u71ctO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Прогресс!\n",
        "\n",
        "Хочется как-то посмотреть, что заинтересовало классификатор. К счастью, сделать это совсем просто:"
      ]
    },
    {
      "metadata": {
        "id": "8W1Ngl-aVuYx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "eli5.show_weights(classifier, vec=vectorizer, top=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0VgCE9tDk-aO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотрим на конкретные примеры его работы:"
      ]
    },
    {
      "metadata": {
        "id": "v-4sVWBOWJpo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer, \n",
        "                     targets=['positive'], target_names=['negative', 'positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AoZhtlYlW-xG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Positive' if test_df['is_positive'].iloc[0] else 'Negative')\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[0], vec=vectorizer, \n",
        "                     targets=['positive'], target_names=['negative', 'positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNNUqZvplhAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотрим на примеры неправильной классификации, наконец:"
      ]
    },
    {
      "metadata": {
        "id": "yf9ZzS8fXKFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = model.predict(test_df['review'])\n",
        "incorrect_pred_index = np.random.choice(np.where(preds != test_df['is_positive'])[0])\n",
        "\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[incorrect_pred_index],\n",
        "                     vec=vectorizer, targets=['positive'], target_names=['negative', 'positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZbFXKNrngP46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Придумываем новые признаки"
      ]
    },
    {
      "metadata": {
        "id": "dz7GSFzIlv4V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tf-idf\n",
        "\n",
        "Сейчас мы на все слова смотрим с одинаковым весом - хотя какие-то из них более редкие, какие-то более частые, и эта частотность - полезная, вообще говоря, информация.\n",
        "\n",
        "Самый простой способ добавить статистическую информацию о частотностях - сделать *tf-idf* взвешивание:\n",
        "\n",
        "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
        "\n",
        "*tf* - term-frequency - частотность слова `t` в конкретном документе `d` (рецензии в нашем случае). Это ровно то, что мы уже считали.\n",
        "\n",
        "*idf* - inverse document-frequency - коэффициент, который тем больше, чем в меньшем числе документов встречалось данное слово. Считается как-нибудь так:\n",
        "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
        "где $n_d$ - число всех документов, а $n_{d(t)}$ - число документов со словом `t`.\n",
        "\n",
        "Использовать его просто - нужно заменить `CountVectorizer` на `TfidfVectorizer`.\n",
        "\n",
        "**Задание** Попробуйте запустить `TfidfVectorizer`. Посмотрите на ошибки, которые он научился исправлять, и на ошибки, которые он начал делать - по сравнению с `CountVectorizer`."
      ]
    },
    {
      "metadata": {
        "id": "p3DjjiJglvT3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(train_df['review'], train_df['is_positive'])\n",
        "\n",
        "eval_model(model, test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xe_CJxQ0tFP9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### N-граммы слов\n",
        "\n",
        "До сих пор мы смотрели на тексты как на мешок слов - но очевидно, что есть разница между `good movie` и `not good movie`.\n",
        "\n",
        "Добавим информацию (хоть какую-то) о последовательностях слов - будем извлекать еще и биграммы слов.\n",
        "\n",
        "В Vectorizer'ах для этого есть параметр `ngram_range=(n_1, n_2)` - он говорит, что нужны n_1-...n_2-граммы.\n",
        "\n",
        "**Задание** Попробуйте увеличенный range и поинтерпретируйте полученный результат."
      ]
    },
    {
      "metadata": {
        "id": "RDpdrT0HuKYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(train_df['review'], train_df['is_positive'])\n",
        "\n",
        "eval_model(model, test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YrBoThj6wl2F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### N-граммы символов\n",
        "\n",
        "Символьные n-граммы дают простой способ выучить полезные корни и суффиксы, не связываясь с этой вашей лингвистикой - только статистика, только хардкор.\n",
        "\n",
        "Например, слово `badass` мы можем представить в виде такой последовательности триграмм:\n",
        "\n",
        "`##b #ba bad ada das ass ss# s##`\n",
        "\n",
        "So interpretable, неправда ли?\n",
        "\n",
        "Реализовать это дело всё так же просто - нужно поставить `analyzer='char'` в вашем любимом Vectorizer'е и выбрать размер `ngram_range`.\n",
        "\n",
        "**Задание** Запилите классификатор на n-граммах символов и визуализируйте его."
      ]
    },
    {
      "metadata": {
        "id": "QFaWmUrGyY3n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(train_df['review'], train_df['is_positive'])\n",
        "\n",
        "eval_model(model, test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9E1E1Bn8yvhq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer, \n",
        "                     targets=['positive'], target_names=['negative', 'positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9fZ6I8mN0VPU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Подключаем лингвистику"
      ]
    },
    {
      "metadata": {
        "id": "YkywIvbp4N-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Лемматизация и стемминг\n",
        "\n",
        "Если присмотреться, можно найти формы одного слова с разной семантической окраской по мнению классификатора. Или нет?\n",
        "\n",
        "**Задание** Найти формы слова с разной семантической окраской.\n",
        "\n",
        "Поверя, что они есть, попробуем что-нибудь с этим сделать.\n",
        "\n",
        "Например, лемматизируем - сведем к начальной форме все слова. Поможет в этом библиотека spacy."
      ]
    },
    {
      "metadata": {
        "id": "S5CTTdtxI5yv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser'])\n",
        "\n",
        "docs = [doc for doc in nlp.pipe(train_df.review.values[:50])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXs8Ia_bqeS0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for token in docs[0]:\n",
        "    print(token.text, token.lemma_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oVXJr_xxqlPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Весь этот процесс очень долгий, поэтому я предподсчитал всё."
      ]
    },
    {
      "metadata": {
        "id": "3ZNz7E5JqrWz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('train_docs.pkl', 'rb') as f:\n",
        "    train_docs = pickle.load(f)\n",
        "    \n",
        "with open('test_docs.pkl', 'rb') as f:\n",
        "    test_docs = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Pfup3O5r30m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Сделайте классификатор на лемматизированных текстах.\n",
        "\n",
        "Более простой способ нормализации слов - использовать стемминг. Он немного тупой, не учитывает контекст, но иногда оказывается даже эффективнее лемматизации - а, главное, быстрее.\n",
        "\n",
        "По сути это просто набор правил, как обрезать слово, чтобы получить основу (stem):"
      ]
    },
    {
      "metadata": {
        "id": "Cr0w_hVyrqFx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(stemmer.stem('become'))\n",
        "print(stemmer.stem('becomes'))\n",
        "print(stemmer.stem('became'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NxGSEqHNsfHy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Попробуйте вместо лемм классифицировать основы."
      ]
    },
    {
      "metadata": {
        "id": "o1NBpkwuspBX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### NER\n",
        "\n",
        "В текстах рецензий очень много именованных сущностей. Вот, например:"
      ]
    },
    {
      "metadata": {
        "id": "Xki8Maf9MrVY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "displacy.render(docs[0], style='ent', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zGhcnuZSteHc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Вообще говоря, почему вдруг какой-нибудь Депп должен нести семантическую окраску? Однако оказывается, что классификатор выучивает, что какие-то имена чаще в положительных рецензиях - или наоборот. Это похоже на переобучение - почему бы не попробовать вырезать сущности?\n",
        "\n",
        "**Задание** Удалите из текстов какие-то из сущностей, пользуясь координатами из запикленных файлов. Описание сущностей можно посмотреть [здесь](https://spacy.io/api/annotation#named-entities). Запустите классификатор."
      ]
    },
    {
      "metadata": {
        "id": "k2fHA70b0zEZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Включаем дип лёрнинг"
      ]
    },
    {
      "metadata": {
        "id": "y1VCrM671XO5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Мы тут пришли deep learning'ом заниматься, а делаем почему-то модель на логистической регрессии. Как так?\n",
        "\n",
        "Попробуем запустить относительно стандартную модель для классификации текстов - сверточная сеть поверх словных эмбеддингов.\n",
        "\n",
        "Разбираться, что это за зверь, будем на следующих занятиях, а пока будем просто им пользоваться :)\n",
        "\n",
        "Каждое предложение нужно представлять набором слов - и сразу же начинаются проблемы. Во-первых, как ограничить длину предложения?\n",
        "\n",
        "Прикинем по гистограмме, какая длина нам подходит:"
      ]
    },
    {
      "metadata": {
        "id": "O477ZV1t1WIO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_, _, hist = plt.hist(train_df.review.apply(lambda text: len(text)), bins='auto')\n",
        "hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UXO4xi0u5m8l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Кроме этого, нужно перенумеровать как-то слова."
      ]
    },
    {
      "metadata": {
        "id": "mIMGE7L-55fs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "words_counter = Counter((word for text in train_df.review for word in text.lower().split()))\n",
        "\n",
        "word2idx = {\n",
        "    '': 0,\n",
        "    '<unk>': 1\n",
        "}\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < <choose the limit>:\n",
        "        break\n",
        "        \n",
        "    word2idx[word] = len(word2idx)\n",
        "    \n",
        "print('Words count', len(word2idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oTptlmd1yD3J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Сконвертируйте данные"
      ]
    },
    {
      "metadata": {
        "id": "ZPP0cYdJ5VkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert(texts, word2idx, max_text_len):\n",
        "    data = np.zeros((len(texts), max_text_len), dtype=np.int)\n",
        "    \n",
        "    <fill the matrix>\n",
        "    return data\n",
        "\n",
        "X_train = convert(train_df.review, word2idx, <your limit>)\n",
        "X_test = convert(test_df.review, word2idx, <your limit>)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AYb-p5ioyLUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Поставим учиться модельку на keras.\n",
        "\n",
        "*Напоминание*: на keras, чтобы обучить модель, нужно\n",
        "1. Определить модель, например:\n",
        "```python \n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='sigmoid', input_dim=NUM_WORDS))\n",
        "```\n",
        "2. Задать функцию потерь и оптимизатор:\n",
        "```python\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "3. Запустить обучение:\n",
        "```python\n",
        "model.fit(X_train, y_train, \n",
        "          batch_size=32,\n",
        "          epochs=3,\n",
        "          validation_data=(X_test, y_test))\n",
        "```\n",
        "\n",
        "В NLP чаще всего ставятся задачи классификации, поэтому нужно запомнить такие функции потерь:\n",
        "\n",
        "*   **categorical_crossentropy** - для многоклассовой классификации, в качестве меток должны передаваться one-hot-encoding вектора\n",
        "*   **sparse_categorical_crossentropy** - аналогично предыдущему, но в качестве меток нужно передавать просто индексы соответствующих классов\n",
        "*   **binary_crossentropy** - для бинарной классификации\n",
        "\n",
        "\n",
        "В качестве оптимизатора обычно используют `sgd` или `adam`.\n"
      ]
    },
    {
      "metadata": {
        "id": "dDjri3167vFf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(word2idx), output_dim=64, input_shape=(X_train.shape[1],)),\n",
        "    Conv1D(filters=128, kernel_size=3, padding='valid', activation='relu', strides=1),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lw93gTmq8gZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, train_df.is_positive, batch_size=128, epochs=10, \n",
        "          validation_data=(X_test, test_df.is_positive))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBGdVRQTyynD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Подсчитайте качество модели на тесте"
      ]
    },
    {
      "metadata": {
        "id": "I-MHfe3by8JW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Немного математики\n",
        "\n",
        "Вспомним, как работает логистическая регрессия, которой мы тут столько пользовались.\n",
        "\n",
        "Это обычная линейная функция\n",
        "\n",
        "$$h_w(x) = \\sigma(w^T x),$$\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "Подсчитайте ее:"
      ]
    },
    {
      "metadata": {
        "id": "PVd7ttkozkFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward(x, w):\n",
        "    <calc h>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "It8mydxfzr-Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Функция потерь \n",
        "$$J(w)  = \\frac{1}{m} (-y^T \\text{log}(h) + (1-y)^T \\text{log}(1 - h)$$\n",
        "\n",
        "Для оптимизации с SGD нужно считать градиент по функции потерь.\n",
        "\n",
        "**Задание** Подсчитайте его."
      ]
    },
    {
      "metadata": {
        "id": "XjLlb4ux0oJ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient(x, h, y):\n",
        "    <calc gradient>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8zhjPhrM1WR-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В итоге получим такой класс:"
      ]
    },
    {
      "metadata": {
        "id": "M6UOJICT020e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True):\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.fit_intercept = fit_intercept\n",
        "    \n",
        "    def __add_intercept(self, X):\n",
        "        # should handle sparse matrix as well\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        return np.concatenate((intercept, X), axis=1)\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "        \n",
        "        # weights initialization\n",
        "        self._w = np.zeros(X.shape[1])\n",
        "        \n",
        "        for i in range(self.num_iter):\n",
        "            h = forward(X, self._w)\n",
        "            grad = gradient(X, h, y)\n",
        "            self.w -= self.lr * gradient\n",
        "    \n",
        "    def predict_prob(self, X):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "    \n",
        "        return forward(X, self._w)\n",
        "    \n",
        "    def predict(self, X, threshold):\n",
        "        return self.predict_prob(X) >= threshold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9coIbUWkn6xs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача задания\n",
        "\n",
        "Сдаем через [форму](https://goo.gl/forms/TWl14R42ddDPvZLU2).\n",
        "\n",
        "И можно заполнить [анонимный опрос](https://goo.gl/forms/RII9cRcwoaJjPr2G3)."
      ]
    }
  ]
}